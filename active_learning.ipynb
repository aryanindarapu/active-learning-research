{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from unet_model import UNet\n",
    "from dataset import *\n",
    "from quantitative_results import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'seed': 310, # random seed\n",
    "  'input_shape': (240, 360), # size of input images (height, width)\n",
    "  'output_shape': (240, 360), # size of output target\n",
    "  'n_train': 40, # number of training images per view\n",
    "  'batch_size': 2, # number of images per optimization step\n",
    "  'lr': 1e-2, # learning rate\n",
    "  'n_epochs': 200, # number of passes through training data\n",
    "  # optionally perform random cropping, specify integer < max(H, W) to use cropping for training\n",
    "  'crop_size': 'None',\n",
    "  # fraction of training epochs to lower learning rate by 1/10, e.g. [0.6, 0.8]\n",
    "  # lowers learning rate at epochs 120 and 160 if we have 200 training epochs\n",
    "  'milestones': [0.8],\n",
    "  'views': ['Almond at Washington North', 'Almond at Washington East'], # list of views for training\n",
    "  'images_path': 'Annotations-Images', # path to RGB images\n",
    "  'gt_path': 'Annotations-GT', # path to ground-truth segmentation masks\n",
    "  'log_path': 'x' # path to directory you make for saving results\n",
    "}\n",
    "\n",
    "# set up model\n",
    "n_channels = 3 # RGB images\n",
    "model = UNet(n_channels)\n",
    "\n",
    "# set up datasets\n",
    "seed = config['seed']\n",
    "np.random.seed(seed) \n",
    "views = config['views']\n",
    "logging_path = config['log_path']\n",
    "if not os.path.exists(logging_path):\n",
    "    os.mkdir(logging_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make initial datasets for each view\n",
    "print('Loading datasets...')\n",
    "datasets = []\n",
    "for v in views:\n",
    "    datasets.append(BFSEvaluationDataset(config['images_path'],\n",
    "                                         config['gt_path'],\n",
    "                                         [v],\n",
    "                                         config['input_shape'],\n",
    "                                         config['output_shape']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_learning import *\n",
    "act = ActiveLearning(model=model, n_init=20, n_train_per_view=40, datasets=datasets, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = act.run_loop(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
