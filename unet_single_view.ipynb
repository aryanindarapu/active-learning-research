{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/aryanindarapu/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6531e+00, -4.3505e+00, -1.8172e+00, -4.2143e+00, -3.1914e+00,\n",
      "         3.4164e-01,  1.0877e+00,  5.9350e+00,  8.0425e+00, -7.0242e-01,\n",
      "        -9.4130e-01, -6.0822e-01, -2.4097e-01, -1.9946e+00, -1.5288e+00,\n",
      "        -3.2656e+00, -5.5800e-01,  1.0524e+00,  1.9211e-01, -4.7202e+00,\n",
      "        -3.3880e+00,  4.3048e+00, -1.0997e+00,  4.6132e+00, -5.7418e-03,\n",
      "        -5.3437e+00, -4.7378e+00, -3.3974e+00, -4.1287e+00,  2.9064e-01,\n",
      "        -3.2955e+00, -6.7051e+00, -4.7232e+00, -4.1778e+00, -2.1859e+00,\n",
      "        -2.9469e+00,  3.0465e+00, -3.5882e+00, -6.3890e+00, -4.4203e+00,\n",
      "        -3.3685e+00, -5.0983e+00, -4.9006e+00, -5.5235e+00, -3.7234e+00,\n",
      "        -4.0204e+00,  2.6998e-01, -4.4702e+00, -5.6617e+00, -5.4880e+00,\n",
      "        -2.6801e+00, -3.2129e+00, -1.6294e+00, -5.2289e+00, -2.7495e+00,\n",
      "        -2.6286e+00, -1.8206e+00, -2.3196e+00, -5.2806e+00, -3.7652e+00,\n",
      "        -3.0987e+00, -4.1421e+00, -5.2531e+00, -4.6504e+00, -3.5815e+00,\n",
      "        -4.0189e+00, -4.0008e+00, -4.5512e+00, -3.2248e+00, -7.7903e+00,\n",
      "        -1.4484e+00, -3.8347e+00, -4.5611e+00, -4.3681e+00,  2.7234e-01,\n",
      "        -4.0162e+00, -4.2136e+00, -5.4524e+00,  1.1744e+00, -4.7785e+00,\n",
      "        -1.8335e+00,  4.1288e-01,  2.2239e+00, -9.9919e-02,  4.8216e+00,\n",
      "        -8.4304e-01,  5.6911e-01, -4.0484e+00, -3.3013e+00,  2.8698e+00,\n",
      "        -1.1419e+00, -9.1690e-01, -2.9284e+00, -2.6097e+00, -1.8214e-01,\n",
      "        -2.5429e+00, -2.1095e+00,  2.2419e+00, -1.6280e+00,  7.4458e+00,\n",
      "         2.3184e+00, -5.7408e+00, -7.4332e-01, -5.4066e+00,  1.5177e+01,\n",
      "        -4.4736e-02,  1.8237e+00, -3.7741e+00,  9.2271e-01, -4.3687e-01,\n",
      "        -1.4003e+00, -4.3026e+00,  6.3781e-01, -1.0808e+00, -1.4173e+00,\n",
      "         2.6194e+00, -3.8418e+00,  1.1598e+00, -2.6876e+00, -3.6103e+00,\n",
      "        -4.9281e+00, -4.1411e+00, -3.3603e+00, -3.4296e+00, -1.4997e+00,\n",
      "        -2.8381e+00, -1.2843e+00,  1.5745e+00, -1.7449e+00,  4.2902e-01,\n",
      "         3.1234e-01, -2.8206e+00,  3.6688e-01, -2.1033e+00,  1.6482e+00,\n",
      "         1.4222e+00, -2.7303e+00, -3.6292e+00,  1.2864e+00, -2.5541e+00,\n",
      "        -2.9663e+00, -4.1575e+00, -3.1954e+00, -4.6487e-01,  1.8916e+00,\n",
      "        -7.4721e-01,  4.5986e+00, -2.5443e+00, -6.2003e+00, -1.3215e+00,\n",
      "        -2.6225e+00,  9.9639e+00,  9.7772e+00,  9.6715e+00,  9.0857e+00,\n",
      "         5.7773e+00,  1.5155e+00,  9.1023e+00,  4.5466e+00, -1.1239e+00,\n",
      "         5.9134e+00, -3.1552e-01, -7.8261e-01, -5.5760e-01, -1.8962e+00,\n",
      "        -4.1288e+00, -1.7884e+00, -7.5252e-01,  3.9682e-01,  7.8177e+00,\n",
      "         3.8134e+00, -1.3543e+00,  1.2913e+00,  6.4669e+00,  5.8514e+00,\n",
      "         2.6018e+00,  2.6898e+00,  1.9930e+00, -2.1234e+00,  4.6571e+00,\n",
      "         3.4012e+00,  1.3347e+00,  4.1266e+00,  1.6413e+00,  3.9784e+00,\n",
      "         6.9870e+00,  7.8652e+00,  2.9290e+00,  6.3692e+00,  6.8548e-01,\n",
      "         6.0998e+00,  7.3568e-01,  5.5853e+00,  3.0740e+00,  5.9059e+00,\n",
      "         3.4023e+00,  4.0194e+00,  2.0428e+00,  1.7228e+00,  8.4634e+00,\n",
      "         7.7994e+00,  2.3141e+00,  4.9181e+00,  1.0704e+01,  6.0487e+00,\n",
      "         3.1511e+00, -1.1049e+00,  9.0660e+00,  6.9668e+00,  2.9854e+00,\n",
      "        -1.5658e+00,  2.3524e-01,  4.7346e+00,  5.5086e-01,  2.8366e+00,\n",
      "         5.1261e+00,  5.5401e+00,  5.3350e+00,  2.5738e+00,  2.5391e+00,\n",
      "         6.2561e+00,  2.6662e-01,  1.0529e+01,  5.5757e+00,  4.3984e+00,\n",
      "         4.9205e-01,  4.5775e+00,  6.5327e+00,  6.2971e+00,  8.2239e+00,\n",
      "         1.1580e+01,  1.0441e+01,  9.1162e+00,  4.5136e+00,  5.0057e-02,\n",
      "         8.2789e+00,  2.7933e+00,  1.2667e+00,  3.9543e+00,  4.6614e+00,\n",
      "         4.2697e+00,  1.8932e+00,  3.7421e+00, -5.2015e-01,  5.4627e+00,\n",
      "         4.3209e+00,  4.8047e+00,  6.4502e+00,  1.1558e+01,  9.4745e+00,\n",
      "         1.0396e+01,  5.1257e+00,  2.2752e+00,  3.1707e+00,  2.5909e+00,\n",
      "         3.2563e+00,  6.0614e+00,  1.1309e+01,  1.6825e+01,  1.4313e+01,\n",
      "         9.6473e+00,  9.2852e+00,  3.8577e+00,  9.4158e+00,  9.2970e+00,\n",
      "         8.3887e+00,  7.5184e+00,  8.6426e+00, -1.6414e+00,  7.3179e+00,\n",
      "         1.1844e+01,  2.4881e+00,  2.1645e+00,  7.3728e+00,  4.0743e+00,\n",
      "        -3.2729e+00, -7.5632e-01,  4.5371e+00,  6.3825e+00,  1.2761e+01,\n",
      "         2.9800e+00,  6.7029e+00,  6.0649e+00,  1.1554e+01,  8.2879e+00,\n",
      "         7.7940e+00,  2.0335e+00,  7.2505e+00, -8.8975e-01,  8.8961e-01,\n",
      "        -2.3303e+00,  1.7753e+00,  3.9600e+00,  6.9571e-01, -2.1025e+00,\n",
      "        -1.3851e+00,  3.9058e+00, -4.8924e+00, -2.5443e+00, -2.1743e+00,\n",
      "        -4.1369e+00, -1.7405e+00, -3.3237e+00, -5.2608e+00, -4.6303e+00,\n",
      "        -4.7160e+00, -4.6914e+00, -4.0146e+00,  2.7552e-01, -1.2765e+00,\n",
      "        -4.7126e-01, -1.7545e+00, -3.1982e+00, -1.6435e+00, -2.2036e+00,\n",
      "        -3.2059e+00, -2.7874e+00, -2.5656e+00, -2.8286e+00, -5.4447e+00,\n",
      "        -5.7600e+00, -7.4010e-02, -1.7109e+00, -3.7817e+00, -1.3318e+00,\n",
      "        -3.8970e+00, -1.4044e+00,  1.6140e+00, -1.3453e+00, -2.1589e+00,\n",
      "         1.0178e+01,  1.2249e+01,  1.3368e+01,  8.9446e+00,  2.5004e+00,\n",
      "         4.3795e+00,  2.5553e+00,  1.1427e+00,  5.2525e+00, -1.1034e+00,\n",
      "         5.1649e-01,  2.3851e+00, -4.6297e-01, -3.0384e+00, -4.8685e+00,\n",
      "         1.8297e+00, -2.2429e+00, -3.4593e+00,  8.8696e+00, -4.5638e-01,\n",
      "        -1.9446e+00, -4.8571e+00, -5.4360e-01,  1.7708e+00, -1.6086e+00,\n",
      "         8.0669e+00,  7.6676e+00,  6.6009e+00,  7.1212e+00,  6.5511e+00,\n",
      "        -1.6199e+00,  3.6741e+00,  1.3183e+00, -4.4796e-01, -2.8097e+00,\n",
      "        -3.1187e+00, -3.4911e+00, -5.1303e+00,  1.1377e+00, -2.9906e+00,\n",
      "         9.2196e-01, -9.2510e-02, -7.5148e-01,  1.4160e+00,  1.3436e+00,\n",
      "        -2.3818e+00, -3.2409e+00,  2.5115e+00,  1.1660e+00, -1.8630e+00,\n",
      "         1.6514e+00, -1.0128e+00, -1.3111e+00,  2.8397e+00,  1.6169e+00,\n",
      "        -6.1281e+00, -7.2739e+00, -1.7869e+00,  2.2953e-01, -2.6277e+00,\n",
      "        -4.7378e+00, -2.5931e-01, -4.3804e+00, -2.6091e+00, -2.7485e+00,\n",
      "        -2.2095e+00, -3.1298e+00, -4.1939e+00, -3.0170e-01, -3.3679e+00,\n",
      "        -3.5106e+00, -1.6222e+00, -1.5633e+00, -3.8200e+00, -3.4389e-01,\n",
      "         1.4091e+00, -5.1334e+00, -1.0952e+00, -3.3511e+00,  8.7169e-01,\n",
      "        -1.5623e+00, -2.1561e+00,  5.2436e+00, -1.4502e+00,  1.3622e+00,\n",
      "        -1.9643e+00, -2.2841e+00,  1.5012e+00,  7.6907e-01, -2.5404e-01,\n",
      "        -1.2807e+00, -1.6901e-02, -1.1180e+00, -2.2726e+00, -2.1818e+00,\n",
      "         8.1014e-01, -2.1304e+00, -1.4641e+00,  7.2842e+00,  4.9355e+00,\n",
      "         3.8114e-01,  1.6081e+00,  1.4207e-01, -2.2375e+00,  2.0593e+00,\n",
      "         1.5662e+00, -7.4137e-01,  3.8722e+00,  1.7906e+00,  2.6670e+00,\n",
      "        -7.0649e-01, -1.4162e+00,  2.3219e-01,  1.4732e+00, -2.0789e+00,\n",
      "        -4.6608e-01, -1.7435e+00, -1.9540e+00,  9.2343e-01, -1.5039e+00,\n",
      "         1.6313e+00, -1.3970e+00,  2.9115e+00, -3.0624e-02,  6.6558e-01,\n",
      "         4.3597e-01, -2.0390e+00,  7.4015e+00,  6.6309e-01,  2.5805e-01,\n",
      "         2.1681e+00, -2.0966e+00,  2.8071e+00,  7.7179e+00, -1.8649e+00,\n",
      "        -3.1810e+00, -3.2157e+00, -3.1040e+00,  6.2041e-01, -5.7294e-01,\n",
      "         2.7674e+00,  2.2801e+00, -2.1331e-02, -2.9130e+00, -1.9868e+00,\n",
      "         2.6699e-02, -5.4558e-01, -1.8165e+00,  3.4987e+00, -4.9991e-01,\n",
      "        -1.6379e+00, -3.1351e+00, -2.5348e+00,  5.4957e-01, -2.9551e+00,\n",
      "        -3.4927e+00, -3.4600e+00,  2.6452e+00, -2.8489e+00,  3.3068e+00,\n",
      "        -1.0797e+00, -1.7570e+00, -1.0633e+00, -1.3794e+00, -1.9415e+00,\n",
      "        -2.9466e+00,  3.5183e+00,  1.8789e+00, -3.0337e+00, -2.2272e+00,\n",
      "        -6.7641e+00,  2.8545e-01, -2.9972e-01, -1.6004e+00,  1.2635e+00,\n",
      "        -2.6329e+00, -2.0945e+00, -4.6474e+00,  2.5814e+00, -1.4716e+00,\n",
      "        -3.2143e+00, -1.6445e+00, -2.8780e+00, -8.7335e-01, -1.2002e+00,\n",
      "         3.5576e+00, -3.4127e+00, -5.1087e-02, -6.7064e-02,  3.8194e+00,\n",
      "        -8.2055e-01,  3.1030e+00,  2.6055e+00,  2.1326e-01, -3.2778e+00,\n",
      "        -3.6279e+00, -1.0141e+00, -5.1832e-01, -2.2252e+00,  1.9740e-02,\n",
      "         3.4781e-01, -1.8625e+00, -1.3475e+00,  6.9910e-01,  3.6843e+00,\n",
      "        -4.7123e+00, -3.5867e+00,  3.2842e+00, -4.1673e+00,  7.3663e+00,\n",
      "        -4.9297e+00, -1.6607e+00, -1.2216e+00,  3.6301e+00, -2.5615e+00,\n",
      "        -6.5167e-01, -7.7884e-01, -4.0890e+00, -3.4347e+00, -6.7509e-01,\n",
      "        -6.0739e+00, -1.9719e+00,  5.0621e+00,  1.6963e+00, -1.1343e+00,\n",
      "        -9.8868e-01,  6.4425e-01,  3.5839e+00, -4.9558e-01, -1.0440e+00,\n",
      "         2.8464e+00, -3.4423e+00,  7.8792e-01, -2.4293e+00, -2.9206e+00,\n",
      "        -3.8922e+00,  3.4876e-01, -1.2449e+00,  3.5581e+00, -1.6783e+00,\n",
      "        -2.2167e+00, -2.4084e+00, -7.1027e-02, -1.4492e+00,  1.3445e-01,\n",
      "        -2.0430e-01, -3.4434e+00, -3.7481e+00,  3.8119e+00, -3.3786e+00,\n",
      "        -1.9093e-01, -1.4250e+00, -1.1209e+00, -2.9558e+00,  1.0028e+00,\n",
      "        -1.5030e-01, -6.3681e-01,  2.0952e+00,  1.5482e+00,  8.5715e-01,\n",
      "        -2.6253e+00, -1.7071e+00, -2.1729e+00,  6.8447e-01, -5.8839e-01,\n",
      "        -1.5405e+00,  2.0318e+00, -5.5361e+00, -1.2917e+00, -2.9692e+00,\n",
      "        -1.6848e+00,  2.9436e+00, -1.6952e+00,  3.1038e-02, -2.6507e+00,\n",
      "         7.6503e-02, -5.9168e-01, -3.0478e+00,  2.7757e+00, -3.6653e-01,\n",
      "        -1.2022e+00,  8.2107e-01, -3.4665e+00, -2.7079e+00,  2.5938e+00,\n",
      "        -6.7904e-01, -5.2500e+00, -2.1233e+00, -2.4535e+00, -3.2205e-01,\n",
      "         4.0266e+00,  3.8202e+00,  1.8239e+00, -2.2377e+00, -2.5470e+00,\n",
      "        -1.8961e+00,  1.1507e+00, -9.0863e-01, -2.9222e+00, -2.6309e-02,\n",
      "        -1.0096e+00,  1.4418e+00, -2.1566e+00, -2.9206e+00, -2.7211e+00,\n",
      "        -1.6706e+00, -2.6514e-01,  1.2232e+00, -3.6349e-01, -1.8894e-02,\n",
      "         6.5338e-01, -1.6350e+00, -2.5687e+00,  2.0967e+00,  1.9325e+00,\n",
      "         7.3038e-02,  6.1249e-01,  4.2801e-01, -1.1372e-01, -3.6632e+00,\n",
      "        -3.4853e+00, -1.7770e+00,  2.9665e-01,  8.1546e-01, -1.0411e+00,\n",
      "        -1.9992e+00, -1.2594e+00,  4.5067e+00,  2.8344e+00, -9.8013e-02,\n",
      "        -1.1884e+00, -3.3622e+00, -1.6943e+00, -2.0767e+00,  4.2992e+00,\n",
      "        -9.0376e-01, -2.7186e+00,  3.9871e+00, -9.7471e-01, -3.6234e+00,\n",
      "         1.0620e+00, -2.9262e+00, -9.6398e-01,  3.1839e+00,  5.6498e-01,\n",
      "        -1.8956e+00,  6.8538e+00, -6.9846e-01, -1.2373e+00, -3.3318e+00,\n",
      "         2.5409e+00,  3.6782e+00, -4.4193e+00, -3.2795e+00,  4.5217e-01,\n",
      "        -6.3557e-01, -4.5141e+00, -5.6394e+00, -1.7874e+00, -5.3225e-01,\n",
      "        -1.8976e+00, -2.4368e+00, -1.5791e+00, -1.5968e+00, -6.6346e-01,\n",
      "        -3.2542e+00,  1.9707e+00, -1.4510e+00, -2.8576e+00,  1.8115e-01,\n",
      "         3.4011e+00,  3.6433e+00, -4.1488e+00,  5.3941e+00,  1.7155e+00,\n",
      "        -1.7607e+00, -1.8127e+00, -5.6651e-01, -1.7138e-01,  2.9694e-01,\n",
      "        -1.2053e+00, -3.1280e-01, -4.4542e-01, -1.6290e+00, -5.1474e-02,\n",
      "        -4.4726e+00,  1.3656e+00, -3.8251e-01, -1.1162e+00,  1.5726e+00,\n",
      "         1.1778e+00,  3.1226e+00,  4.7183e+00,  4.2658e+00, -2.8885e+00,\n",
      "         1.1531e+00, -6.0749e+00, -2.5053e-01,  2.7380e+00, -3.5834e+00,\n",
      "        -2.4107e+00,  1.4842e+00, -4.2244e-01,  1.2599e+00, -2.4290e+00,\n",
      "        -1.8403e+00, -8.0502e-01, -9.5764e-02,  1.6516e+00, -5.9440e+00,\n",
      "        -1.2061e+00, -1.7422e+00,  3.3496e+00, -4.4659e+00,  5.2483e+00,\n",
      "        -1.5209e+00, -2.0713e+00, -6.2343e-01,  1.6578e+00,  3.8215e+00,\n",
      "         3.9360e+00, -6.6866e-01,  3.6884e-01,  1.1271e+00, -1.4996e+00,\n",
      "        -1.2209e+00, -2.5421e-01, -6.7888e-01, -1.9382e+00, -1.2613e+00,\n",
      "         2.6638e+00,  1.8173e+00, -2.6642e+00, -7.8372e-01, -8.0335e-02,\n",
      "         2.2117e+00, -2.9847e+00, -3.8203e-01,  2.9803e+00,  2.0670e+00,\n",
      "         4.8434e-01, -1.3782e+00, -1.1167e+00,  4.7222e-01, -3.2166e+00,\n",
      "        -1.0120e+00, -3.0073e+00, -3.4002e+00,  2.6437e+00, -3.6960e+00,\n",
      "         4.2053e-02, -8.5282e-01,  4.7455e+00, -1.7194e+00, -1.6682e-01,\n",
      "         1.2189e-01, -2.5808e-01, -6.6408e-01, -6.2078e-01, -1.6177e+00,\n",
      "         1.2100e+00,  2.0241e+00,  1.0516e+00,  7.0260e-01,  2.2579e+00,\n",
      "         1.9435e+00, -1.1773e+00,  2.3265e+00, -3.1482e+00,  1.5769e+00,\n",
      "         1.5495e+00, -1.5095e+00,  1.1099e+00, -2.7918e+00,  9.3726e-01,\n",
      "         5.4669e+00,  3.5356e+00, -2.0647e-01, -2.0895e+00, -1.0241e+00,\n",
      "         6.9857e-01,  2.5615e+00, -7.1754e-02,  5.4015e-01,  7.8100e-01,\n",
      "         1.9774e-01, -1.1075e+00, -2.0489e+00,  2.8353e-01, -2.1950e+00,\n",
      "         1.7179e-01, -3.3436e+00, -1.7189e+00, -1.1354e+00,  4.3628e-01,\n",
      "        -6.8839e-01, -1.3995e+00, -1.2938e+00, -7.0999e-01, -2.6531e+00,\n",
      "        -1.7930e+00, -6.2157e-01, -1.3453e+00, -4.4531e+00, -6.2879e-01,\n",
      "        -7.9792e-01,  7.6609e-01,  1.3241e+00, -1.0893e+00, -2.8454e+00,\n",
      "         1.3747e+00, -9.1034e-01,  9.1862e-01, -6.4992e-01, -4.1809e-01,\n",
      "         1.2721e+00, -4.1330e-01, -2.4140e-01, -3.4234e+00,  3.4303e-01,\n",
      "         5.5246e+00,  1.5737e-01,  8.8572e+00, -2.8212e+00, -1.6723e+00,\n",
      "        -5.7758e-01, -3.0367e+00, -2.5280e+00,  1.6939e-01, -3.5722e-01,\n",
      "        -2.5129e+00,  2.8057e+00, -5.4940e-01, -1.3355e+00, -2.5097e+00,\n",
      "         1.4674e+00, -8.7753e-01, -1.3430e+00, -5.1088e-01, -1.4248e+00,\n",
      "         9.4306e-01, -2.6515e+00, -2.7975e-01, -3.0502e+00, -4.6979e+00,\n",
      "        -4.0597e+00,  2.0581e+00, -1.1199e+00, -1.0410e+00,  2.3181e+00,\n",
      "         1.0971e+00, -4.0029e+00,  5.7723e-01, -2.6048e+00, -3.5901e+00,\n",
      "        -9.2429e-01, -1.8759e+00, -2.8008e+00,  8.2150e-01, -2.2683e+00,\n",
      "        -1.4691e+00, -1.2846e-01,  1.0042e-01, -2.6058e+00, -2.6467e+00,\n",
      "         1.4808e+00,  2.9393e+00,  1.6651e+00,  4.1545e-01,  1.4050e+00,\n",
      "        -1.8840e-01, -4.0952e+00, -2.7387e-01,  6.0092e-01,  1.4788e+00,\n",
      "         1.3376e+00, -2.3120e+00, -1.7013e+00,  1.2680e+00, -5.4642e-01,\n",
      "        -1.2299e+00,  1.5726e+00,  4.5007e+00, -5.1495e+00, -3.4774e+00,\n",
      "        -2.8654e+00,  9.2763e-01, -1.3225e+00,  2.2688e+00, -4.2309e-01,\n",
      "        -1.8813e+00,  1.9572e+00, -3.4873e+00, -3.1902e+00, -2.2388e+00,\n",
      "        -2.8536e+00, -2.5456e+00, -7.9504e-01, -3.3175e+00,  6.8583e+00,\n",
      "        -4.1436e+00, -3.9959e+00, -4.4752e-01, -4.1032e+00, -2.9127e+00,\n",
      "         1.0799e+00, -2.7834e+00,  4.3743e+00,  1.2949e+00, -2.0735e+00,\n",
      "        -2.7583e+00, -3.6715e+00, -5.4815e-01, -2.3058e+00, -3.2384e+00,\n",
      "        -2.2241e+00, -4.7857e-01, -4.7532e-01, -2.4020e+00, -1.0765e+00,\n",
      "        -3.3852e+00, -4.4681e+00, -3.9925e+00, -1.1987e+00, -3.8723e+00,\n",
      "        -3.1125e+00, -6.8727e-01, -1.6097e+00,  2.3063e+00, -5.1035e+00,\n",
      "        -4.0172e+00,  3.2436e-01, -5.2703e+00, -2.9099e+00, -4.0322e+00,\n",
      "        -1.7708e+00, -3.2843e+00, -2.3601e+00,  5.0528e+00, -9.2875e-02,\n",
      "        -1.7664e+00,  4.2515e+00,  5.0532e-01, -1.4602e+00,  5.5996e-01,\n",
      "         2.1899e+00, -3.9130e+00, -1.2206e+00, -2.9555e+00, -4.3260e-01,\n",
      "        -1.4437e+00,  9.8381e-01,  3.3481e+00, -4.2170e+00,  2.3720e+00,\n",
      "        -1.7488e-01, -4.8491e+00,  2.7957e+00, -3.9669e+00,  5.0905e-01,\n",
      "        -1.8495e-01, -6.4006e-01, -1.1009e-01, -1.2967e+00, -2.5475e+00,\n",
      "         2.8983e+00, -2.8059e+00, -3.2593e+00,  8.9075e-01,  2.3235e+00],\n",
      "       device='mps:0')\n",
      "tensor([6.8397e-09, 4.6082e-10, 5.8043e-09, 5.2806e-10, 1.4687e-09, 5.0271e-08,\n",
      "        1.0601e-07, 1.3505e-05, 1.1111e-04, 1.7697e-08, 1.3936e-08, 1.9445e-08,\n",
      "        2.8074e-08, 4.8609e-09, 7.7446e-09, 1.3636e-09, 2.0446e-08, 1.0233e-07,\n",
      "        4.3289e-08, 3.1840e-10, 1.2066e-09, 2.6455e-06, 1.1895e-08, 3.6010e-06,\n",
      "        3.5519e-08, 1.7069e-10, 3.1286e-10, 1.1953e-09, 5.7530e-10, 4.7772e-08,\n",
      "        1.3235e-09, 4.3750e-11, 3.1746e-10, 5.4774e-10, 4.0143e-09, 1.8755e-09,\n",
      "        7.5170e-07, 9.8765e-10, 6.0014e-11, 4.2979e-10, 1.2303e-09, 2.1816e-10,\n",
      "        2.6585e-10, 1.4260e-10, 8.6282e-10, 6.4110e-10, 4.6795e-08, 4.0887e-10,\n",
      "        1.2420e-10, 1.4775e-10, 2.4490e-09, 1.4374e-09, 7.0034e-09, 1.9145e-10,\n",
      "        2.2848e-09, 2.5784e-09, 5.7844e-09, 3.5120e-09, 1.8181e-10, 8.2745e-10,\n",
      "        1.6114e-09, 5.6761e-10, 1.8687e-10, 3.4142e-10, 9.9432e-10, 6.4207e-10,\n",
      "        6.5378e-10, 3.7705e-10, 1.4204e-09, 1.4779e-11, 8.3926e-09, 7.7193e-10,\n",
      "        3.7334e-10, 4.5279e-10, 4.6906e-08, 6.4377e-10, 5.2843e-10, 1.5311e-10,\n",
      "        1.1561e-07, 3.0038e-10, 5.7106e-09, 5.3983e-08, 3.3021e-07, 3.2326e-08,\n",
      "        4.4354e-06, 1.5375e-08, 6.3111e-08, 6.2337e-10, 1.3159e-09, 6.2990e-07,\n",
      "        1.1403e-08, 1.4281e-08, 1.9106e-09, 2.6276e-09, 2.9775e-08, 2.8091e-09,\n",
      "        4.3331e-09, 3.3621e-07, 7.0129e-09, 6.1182e-05, 3.6292e-07, 1.1475e-10,\n",
      "        1.6987e-08, 1.6028e-10, 1.3938e-01, 3.4160e-08, 2.2129e-07, 8.2012e-10,\n",
      "        8.9883e-08, 2.3079e-08, 8.8066e-09, 4.8343e-10, 6.7600e-08, 1.2122e-08,\n",
      "        8.6581e-09, 4.9038e-07, 7.6647e-10, 1.1393e-07, 2.4306e-09, 9.6612e-10,\n",
      "        2.5865e-10, 5.6819e-10, 1.2404e-09, 1.1574e-09, 7.9730e-09, 2.0910e-09,\n",
      "        9.8900e-09, 1.7248e-07, 6.2396e-09, 5.4862e-08, 4.8820e-08, 2.1280e-09,\n",
      "        5.1556e-08, 4.3601e-09, 1.8567e-07, 1.4811e-07, 2.3291e-09, 9.4802e-10,\n",
      "        1.2931e-07, 2.7780e-09, 1.8395e-09, 5.5894e-10, 1.4629e-09, 2.2442e-08,\n",
      "        2.3685e-07, 1.6921e-08, 3.5488e-06, 2.8053e-09, 7.2476e-11, 9.5282e-09,\n",
      "        2.5942e-09, 7.5895e-04, 6.2971e-04, 5.6656e-04, 3.1536e-04, 1.1535e-05,\n",
      "        1.6260e-07, 3.2064e-04, 3.3691e-06, 1.1611e-08, 1.3216e-05, 2.6057e-08,\n",
      "        1.6333e-08, 2.0454e-08, 5.3632e-09, 5.7519e-10, 5.9737e-09, 1.6832e-08,\n",
      "        5.3124e-08, 8.8743e-05, 1.6183e-06, 9.2215e-09, 1.2994e-07, 2.2986e-05,\n",
      "        1.2421e-05, 4.8183e-07, 5.2617e-07, 2.6211e-07, 4.2733e-09, 3.7627e-06,\n",
      "        1.0716e-06, 1.3571e-07, 2.2137e-06, 1.8440e-07, 1.9087e-06, 3.8669e-05,\n",
      "        9.3062e-05, 6.6834e-07, 2.0849e-05, 7.0900e-08, 1.5925e-05, 7.4550e-08,\n",
      "        9.5196e-06, 7.7260e-07, 1.3118e-05, 1.0728e-06, 1.9886e-06, 2.7551e-07,\n",
      "        2.0006e-07, 1.6925e-04, 8.7135e-05, 3.6138e-07, 4.8848e-06, 1.5911e-03,\n",
      "        1.5131e-05, 8.3455e-07, 1.1833e-08, 3.0921e-04, 3.7895e-05, 7.0715e-07,\n",
      "        7.4632e-09, 4.5197e-08, 4.0660e-06, 6.1970e-08, 6.0938e-07, 6.0143e-06,\n",
      "        9.0985e-06, 7.4113e-06, 4.6851e-07, 4.5257e-07, 1.8618e-05, 4.6638e-08,\n",
      "        1.3357e-03, 9.4285e-06, 2.9049e-06, 5.8431e-08, 3.4748e-06, 2.4552e-05,\n",
      "        1.9397e-05, 1.3322e-04, 3.8189e-03, 1.2226e-03, 3.2513e-04, 3.2596e-06,\n",
      "        3.7557e-08, 1.4074e-04, 5.8353e-07, 1.2678e-07, 1.8633e-06, 3.7790e-06,\n",
      "        2.5541e-06, 2.3722e-07, 1.5070e-06, 2.1235e-08, 8.4210e-06, 2.6884e-06,\n",
      "        4.3611e-06, 2.2606e-05, 3.7377e-03, 4.6523e-04, 1.1689e-03, 6.0120e-06,\n",
      "        3.4759e-07, 8.5106e-07, 4.7659e-07, 9.2711e-07, 1.5324e-05, 2.9141e-03,\n",
      "        7.2448e-01, 5.8750e-02, 5.5299e-04, 3.8498e-04, 1.6918e-06, 4.3870e-04,\n",
      "        3.8956e-04, 1.5708e-04, 6.5791e-05, 2.0247e-04, 6.9196e-09, 5.3836e-05,\n",
      "        4.9747e-03, 4.3003e-07, 3.1115e-07, 5.6877e-05, 2.1009e-06, 1.3537e-09,\n",
      "        1.6768e-08, 3.3371e-06, 2.1127e-05, 1.2450e-02, 7.0332e-07, 2.9105e-05,\n",
      "        1.5378e-05, 3.7206e-03, 1.4202e-04, 8.6664e-05, 2.7294e-07, 5.0325e-05,\n",
      "        1.4674e-08, 8.6956e-08, 3.4746e-09, 2.1084e-07, 1.8739e-06, 7.1630e-08,\n",
      "        4.3637e-09, 8.9419e-09, 1.7752e-06, 2.6805e-10, 2.8054e-09, 4.0611e-09,\n",
      "        5.7056e-10, 6.2668e-09, 1.2868e-09, 1.8544e-10, 3.4836e-10, 3.1977e-10,\n",
      "        3.2773e-10, 6.4478e-10, 4.7055e-08, 9.9669e-09, 2.2299e-08, 6.1797e-09,\n",
      "        1.4588e-09, 6.9051e-09, 3.9439e-09, 1.4476e-09, 2.1998e-09, 2.7462e-09,\n",
      "        2.1110e-09, 1.5429e-10, 1.1257e-10, 3.3175e-08, 6.4554e-09, 8.1395e-10,\n",
      "        9.4308e-09, 7.2524e-10, 8.7705e-09, 1.7944e-07, 9.3043e-09, 4.1245e-09,\n",
      "        9.4008e-04, 7.4557e-03, 2.2830e-02, 2.7387e-04, 4.3539e-07, 2.8505e-06,\n",
      "        4.5996e-07, 1.1200e-07, 6.8244e-06, 1.1851e-08, 5.9876e-08, 3.8795e-07,\n",
      "        2.2485e-08, 1.7116e-09, 2.7452e-10, 2.2264e-07, 3.7921e-09, 1.1236e-09,\n",
      "        2.5407e-04, 2.2633e-08, 5.1098e-09, 2.7768e-10, 2.0743e-08, 2.0990e-07,\n",
      "        7.1504e-09, 1.1386e-04, 7.6374e-05, 2.6284e-05, 4.4222e-05, 2.5007e-05,\n",
      "        7.0700e-09, 1.4079e-06, 1.3350e-07, 2.2824e-08, 2.1515e-09, 1.5796e-09,\n",
      "        1.0884e-09, 2.1129e-10, 1.1145e-07, 1.7953e-09, 8.9816e-08, 3.2567e-08,\n",
      "        1.6849e-08, 1.4721e-07, 1.3691e-07, 3.3003e-09, 1.3978e-09, 4.4021e-07,\n",
      "        1.1463e-07, 5.5446e-09, 1.8627e-07, 1.2974e-08, 9.6283e-09, 6.1124e-07,\n",
      "        1.7996e-07, 7.7905e-11, 2.4770e-11, 5.9827e-09, 4.4940e-08, 2.5808e-09,\n",
      "        3.1287e-10, 2.7563e-08, 4.4726e-10, 2.6293e-09, 2.2872e-09, 3.9210e-09,\n",
      "        1.5620e-09, 5.3895e-10, 2.6419e-08, 1.2311e-09, 1.0673e-09, 7.0538e-09,\n",
      "        7.4817e-09, 7.8330e-10, 2.5328e-08, 1.4619e-07, 2.1065e-10, 1.1949e-08,\n",
      "        1.2519e-09, 8.5412e-08, 7.4895e-09, 4.1359e-09, 6.7644e-06, 8.3780e-09,\n",
      "        1.3949e-07, 5.0101e-09, 3.6390e-09, 1.6028e-07, 7.7082e-08, 2.7709e-08,\n",
      "        9.9253e-09, 3.5124e-08, 1.1679e-08, 3.6811e-09, 4.0311e-09, 8.0313e-08,\n",
      "        4.2435e-09, 8.2625e-09, 5.2054e-05, 4.9706e-06, 5.2297e-08, 1.7837e-07,\n",
      "        4.1176e-08, 3.8125e-09, 2.8008e-07, 1.7105e-07, 1.7021e-08, 1.7165e-06,\n",
      "        2.1408e-07, 5.1427e-07, 1.7625e-08, 8.6672e-09, 4.5060e-08, 1.5586e-07,\n",
      "        4.4680e-09, 2.2415e-08, 6.2480e-09, 5.0620e-09, 8.9948e-08, 7.9400e-09,\n",
      "        1.8257e-07, 8.8359e-09, 6.5672e-07, 3.4646e-08, 6.9503e-08, 5.5244e-08,\n",
      "        4.6496e-09, 5.8529e-05, 6.9331e-08, 4.6240e-08, 3.1229e-07, 4.3893e-09,\n",
      "        5.9164e-07, 8.0310e-05, 5.5340e-09, 1.4841e-09, 1.4335e-09, 1.6028e-09,\n",
      "        6.6434e-08, 2.0143e-08, 5.6862e-07, 3.4930e-07, 3.4969e-08, 1.9402e-09,\n",
      "        4.8990e-09, 3.6690e-08, 2.0702e-08, 5.8085e-09, 1.1815e-06, 2.1669e-08,\n",
      "        6.9445e-09, 1.5538e-09, 2.8321e-09, 6.1891e-08, 1.8602e-09, 1.0867e-09,\n",
      "        1.1228e-09, 5.0320e-07, 2.0687e-09, 9.7520e-07, 1.2135e-08, 6.1647e-09,\n",
      "        1.2335e-08, 8.9926e-09, 5.1257e-09, 1.8760e-09, 1.2048e-06, 2.3386e-07,\n",
      "        1.7195e-09, 3.8522e-09, 4.1243e-11, 4.7525e-08, 2.6472e-08, 7.2097e-09,\n",
      "        1.2638e-07, 2.5674e-09, 4.3985e-09, 3.4244e-10, 4.7210e-07, 8.2006e-09,\n",
      "        1.4355e-09, 6.8985e-09, 2.0093e-09, 1.4916e-08, 1.0757e-08, 1.2532e-06,\n",
      "        1.1772e-09, 3.3944e-08, 3.3406e-08, 1.6282e-06, 1.5725e-08, 7.9537e-07,\n",
      "        4.8360e-07, 4.4215e-08, 1.3471e-09, 9.4927e-10, 1.2958e-08, 2.1274e-08,\n",
      "        3.8597e-09, 3.6435e-08, 5.0582e-08, 5.5471e-09, 9.2837e-09, 7.1873e-08,\n",
      "        1.4223e-06, 3.2093e-10, 9.8915e-10, 9.5336e-07, 5.5348e-10, 5.6508e-05,\n",
      "        2.5824e-10, 6.7878e-09, 1.0530e-08, 1.3473e-06, 2.7575e-09, 1.8618e-08,\n",
      "        1.6395e-08, 5.9859e-10, 1.1515e-09, 1.8187e-08, 8.2239e-11, 4.9724e-09,\n",
      "        5.6413e-06, 1.9482e-07, 1.1490e-08, 1.3291e-08, 6.8037e-08, 1.2865e-06,\n",
      "        2.1763e-08, 1.2576e-08, 6.1532e-07, 1.1428e-09, 7.8548e-08, 3.1471e-09,\n",
      "        1.9256e-09, 7.2877e-10, 5.0631e-08, 1.0288e-08, 1.2538e-06, 6.6693e-09,\n",
      "        3.8925e-09, 3.2137e-09, 3.3274e-08, 8.3864e-09, 4.0864e-08, 2.9122e-08,\n",
      "        1.1416e-09, 8.4174e-10, 1.6160e-06, 1.2180e-09, 2.9514e-08, 8.5913e-09,\n",
      "        1.1645e-08, 1.8590e-09, 9.7376e-08, 3.0738e-08, 1.8897e-08, 2.9034e-07,\n",
      "        1.6801e-07, 8.4179e-08, 2.5870e-09, 6.4799e-09, 4.0668e-09, 7.0829e-08,\n",
      "        1.9834e-08, 7.6542e-09, 2.7248e-07, 1.4082e-10, 9.8170e-09, 1.8342e-09,\n",
      "        6.6259e-09, 6.7815e-07, 6.5571e-09, 3.6849e-08, 2.5220e-09, 3.8563e-08,\n",
      "        1.9769e-08, 1.6955e-09, 5.7336e-07, 2.4761e-08, 1.0735e-08, 8.1196e-08,\n",
      "        1.1155e-09, 2.3819e-09, 4.7798e-07, 1.8115e-08, 1.8746e-10, 4.2738e-09,\n",
      "        3.0719e-09, 2.5887e-08, 2.0031e-06, 1.6294e-06, 2.2133e-07, 3.8118e-09,\n",
      "        2.7977e-09, 5.3637e-09, 1.1290e-07, 1.4399e-08, 1.9225e-09, 3.4795e-08,\n",
      "        1.3016e-08, 1.5105e-07, 4.1339e-09, 1.9255e-09, 2.3507e-09, 6.7207e-09,\n",
      "        2.7403e-08, 1.2139e-07, 2.4836e-08, 3.5054e-08, 6.8660e-08, 6.9643e-09,\n",
      "        2.7377e-09, 2.9075e-07, 2.4674e-07, 3.8430e-08, 6.5910e-08, 5.4806e-08,\n",
      "        3.1883e-08, 9.1633e-10, 1.0947e-09, 6.0424e-09, 4.8060e-08, 8.0742e-08,\n",
      "        1.2613e-08, 4.8384e-09, 1.0139e-08, 3.2373e-06, 6.0802e-07, 3.2388e-08,\n",
      "        1.0885e-08, 1.2381e-09, 6.5633e-09, 4.4777e-09, 2.6308e-06, 1.4469e-08,\n",
      "        2.3565e-09, 1.9254e-06, 1.3478e-08, 9.5355e-10, 1.0332e-07, 1.9148e-09,\n",
      "        1.3624e-08, 8.6242e-07, 6.2852e-08, 5.3668e-09, 3.3846e-05, 1.7767e-08,\n",
      "        1.0366e-08, 1.2763e-09, 4.5335e-07, 1.4138e-06, 4.3019e-10, 1.3449e-09,\n",
      "        5.6147e-08, 1.8920e-08, 3.9131e-10, 1.2699e-10, 5.9802e-09, 2.0980e-08,\n",
      "        5.3558e-09, 3.1236e-09, 7.3650e-09, 7.2354e-09, 1.8400e-08, 1.3793e-09,\n",
      "        2.5635e-07, 8.3714e-09, 2.0507e-09, 4.2817e-08, 1.0716e-06, 1.3653e-06,\n",
      "        5.6383e-10, 7.8632e-06, 1.9861e-07, 6.1418e-09, 5.8305e-09, 2.0273e-08,\n",
      "        3.0097e-08, 4.8074e-08, 1.0703e-08, 2.6128e-08, 2.2883e-08, 7.0061e-09,\n",
      "        3.3931e-08, 4.0789e-10, 1.3997e-07, 2.4368e-08, 1.1700e-08, 1.7215e-07,\n",
      "        1.1600e-07, 8.1106e-07, 4.0002e-06, 2.5442e-06, 1.9883e-09, 1.1317e-07,\n",
      "        8.2160e-11, 2.7806e-08, 5.5211e-07, 9.9240e-10, 3.2061e-09, 1.5759e-07,\n",
      "        2.3415e-08, 1.2593e-07, 3.1481e-09, 5.6718e-09, 1.5971e-08, 3.2461e-08,\n",
      "        1.8630e-07, 9.3653e-11, 1.0694e-08, 6.2566e-09, 1.0178e-06, 4.1061e-10,\n",
      "        6.7960e-06, 7.8063e-09, 4.5019e-09, 1.9151e-08, 1.8747e-07, 1.6315e-06,\n",
      "        1.8295e-06, 1.8304e-08, 5.1658e-08, 1.1027e-07, 7.9739e-09, 1.0537e-08,\n",
      "        2.7704e-08, 1.8118e-08, 5.1430e-09, 1.0120e-08, 5.1263e-07, 2.1988e-07,\n",
      "        2.4884e-09, 1.6315e-08, 3.2965e-08, 3.2620e-07, 1.8060e-09, 2.4380e-08,\n",
      "        7.0354e-07, 2.8226e-07, 5.7982e-08, 9.0036e-09, 1.1695e-08, 5.7283e-08,\n",
      "        1.4321e-09, 1.2985e-08, 1.7657e-09, 1.1919e-09, 5.0245e-07, 8.8672e-10,\n",
      "        3.7257e-08, 1.5226e-08, 4.1105e-06, 6.4005e-09, 3.0234e-08, 4.0354e-08,\n",
      "        2.7597e-08, 1.8388e-08, 1.9202e-08, 7.0859e-09, 1.1980e-07, 2.7039e-07,\n",
      "        1.0225e-07, 7.2125e-08, 3.4162e-07, 2.4946e-07, 1.1006e-08, 3.6589e-07,\n",
      "        1.5336e-09, 1.7289e-07, 1.6822e-07, 7.8953e-09, 1.0838e-07, 2.1902e-09,\n",
      "        9.1200e-08, 8.4568e-06, 1.2259e-06, 2.9059e-08, 4.4206e-09, 1.2829e-08,\n",
      "        7.1835e-08, 4.6281e-07, 3.3250e-08, 6.1310e-08, 7.8007e-08, 4.3534e-08,\n",
      "        1.1802e-08, 4.6038e-09, 4.7434e-08, 3.9780e-09, 4.2419e-08, 1.2614e-09,\n",
      "        6.4039e-09, 1.1478e-08, 5.5262e-08, 1.7947e-08, 8.8132e-09, 9.7958e-09,\n",
      "        1.7563e-08, 2.5160e-09, 5.9463e-09, 1.9187e-08, 9.3045e-09, 4.1590e-10,\n",
      "        1.9049e-08, 1.6085e-08, 7.6852e-08, 1.3427e-07, 1.2019e-08, 2.0758e-09,\n",
      "        1.4124e-07, 1.4375e-08, 8.9516e-08, 1.8650e-08, 2.3517e-08, 1.2747e-07,\n",
      "        2.3630e-08, 2.8061e-08, 1.1646e-09, 5.0341e-08, 8.9593e-06, 4.1811e-08,\n",
      "        2.5095e-04, 2.1267e-09, 6.7094e-09, 2.0050e-08, 1.7145e-09, 2.8512e-09,\n",
      "        4.2317e-08, 2.4993e-08, 2.8948e-09, 5.9081e-07, 2.0623e-08, 9.3964e-09,\n",
      "        2.9040e-09, 1.5497e-07, 1.4854e-08, 9.3255e-09, 2.1433e-08, 8.5935e-09,\n",
      "        9.1731e-08, 2.5202e-09, 2.7006e-08, 1.6915e-09, 3.2558e-10, 6.1639e-10,\n",
      "        2.7975e-07, 1.1657e-08, 1.2614e-08, 3.6280e-07, 1.0700e-07, 6.5241e-10,\n",
      "        6.3626e-08, 2.6405e-09, 9.8583e-10, 1.4175e-08, 5.4732e-09, 2.1706e-09,\n",
      "        8.1231e-08, 3.6968e-09, 8.2210e-09, 3.1416e-08, 3.9497e-08, 2.6380e-09,\n",
      "        2.5321e-09, 1.5705e-07, 6.7528e-07, 1.8884e-07, 5.4122e-08, 1.4559e-07,\n",
      "        2.9589e-08, 5.9486e-10, 2.7165e-08, 6.5152e-08, 1.5674e-07, 1.3610e-07,\n",
      "        3.5387e-09, 6.5177e-09, 1.2695e-07, 2.0684e-08, 1.0443e-08, 1.7216e-07,\n",
      "        3.2180e-06, 2.0728e-10, 1.1034e-09, 2.0349e-09, 9.0326e-08, 9.5190e-09,\n",
      "        3.4538e-07, 2.3399e-08, 5.4437e-09, 2.5289e-07, 1.0925e-09, 1.4705e-09,\n",
      "        3.8075e-09, 2.0590e-09, 2.8016e-09, 1.6131e-08, 1.2947e-09, 3.3999e-05,\n",
      "        5.6679e-10, 6.5696e-10, 2.2835e-08, 5.9014e-10, 1.9407e-09, 1.0519e-07,\n",
      "        2.2088e-09, 2.8358e-06, 1.3041e-07, 4.4921e-09, 2.2649e-09, 9.0872e-10,\n",
      "        2.0649e-08, 3.5608e-09, 1.4013e-09, 3.8638e-09, 2.2137e-08, 2.2208e-08,\n",
      "        3.2343e-09, 1.2173e-08, 1.2100e-09, 4.0970e-10, 6.5921e-10, 1.0773e-08,\n",
      "        7.4341e-10, 1.5893e-09, 1.7967e-08, 7.1424e-09, 3.5857e-07, 2.1702e-10,\n",
      "        6.4314e-10, 4.9410e-08, 1.8370e-10, 1.9462e-09, 6.3356e-10, 6.0799e-09,\n",
      "        1.3384e-09, 3.3728e-09, 5.5893e-06, 3.2555e-08, 6.1068e-09, 2.5082e-06,\n",
      "        5.9211e-08, 8.2942e-09, 6.2537e-08, 3.1917e-07, 7.1375e-10, 1.0540e-08,\n",
      "        1.8595e-09, 2.3178e-08, 8.4327e-09, 9.5546e-08, 1.0163e-06, 5.2667e-10,\n",
      "        3.8289e-07, 2.9991e-08, 2.7990e-10, 5.8493e-07, 6.7628e-10, 5.9433e-08,\n",
      "        2.9691e-08, 1.8835e-08, 3.1999e-08, 9.7682e-09, 2.7963e-09, 6.4813e-07,\n",
      "        2.1595e-09, 1.3723e-09, 8.7056e-08, 3.6479e-07], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "# model.to(mps_device)\n",
    "model.eval()\n",
    "\n",
    "input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.backends.mps.is_available():\n",
    "    input_batch = input_batch.to(mps_device)\n",
    "    model.to(mps_device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from supervised_train import train\n",
    "from unet_model import UNet\n",
    "from dataset import *\n",
    "from quantitative_results import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'seed': 1, # random seed\n",
    "  'input_shape': (240, 360), # size of input images (height, width)\n",
    "  'output_shape': (240, 360), # size of output target\n",
    "  'n_train': 5, # number of training images per view\n",
    "  'batch_size': 2, # number of images per optimization step\n",
    "  'lr': 1e-2, # learning rate\n",
    "  'n_epochs': 200, # number of passes through training data\n",
    "  # optionally perform random cropping, specify integer < max(H, W) to use cropping for training\n",
    "  'crop_size': 'None',\n",
    "  # fraction of training epochs to lower learning rate by 1/10, e.g. [0.6, 0.8]\n",
    "  # lowers learning rate at epochs 120 and 160 if we have 200 training epochs\n",
    "  'milestones': [0.8],\n",
    "  'views': ['Almond at Washington North'], # list of views for training\n",
    "  'images_path': 'Annotations-Images', # path to RGB images\n",
    "  'gt_path': 'Annotations-GT', # path to ground-truth segmentation masks\n",
    "  'log_path': 'Almond at Washington North' # path to directory you make for saving results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "[(<dataset.BFSEvaluationDataset object at 0x29b4d43d0>, array([27, 35, 40, 38,  2]))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(logging_path, \u001b[39m'\u001b[39m\u001b[39mconfig.json\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     41\u001b[0m     json\u001b[39m.\u001b[39mdump(config, f)\n\u001b[0;32m---> 44\u001b[0m model, stat_dict \u001b[39m=\u001b[39m train(model, train_dataset, val_dataset, config)  \n\u001b[1;32m     45\u001b[0m \u001b[39m# saving and clean-up\u001b[39;00m\n\u001b[1;32m     46\u001b[0m torch\u001b[39m.\u001b[39msave(model, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(logging_path, \u001b[39m'\u001b[39m\u001b[39mmodel.pt\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/Repos/projects/u_net_baseline/supervised_train.py:79\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, val_dataset, config)\u001b[0m\n\u001b[1;32m     76\u001b[0m train_blob_recalls\u001b[39m.\u001b[39mappend(train_blob_recall[\u001b[39m0\u001b[39m]) \n\u001b[1;32m     78\u001b[0m \u001b[39m# val metrics\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m val_precision, val_recall, val_f_measure, val_blob_recall \u001b[39m=\u001b[39m model_metrics(val_dataset, model, thresholds, device)\n\u001b[1;32m     80\u001b[0m val_precisions\u001b[39m.\u001b[39mappend(val_precision[\u001b[39m0\u001b[39m])\n\u001b[1;32m     81\u001b[0m val_recalls\u001b[39m.\u001b[39mappend(val_recall[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/Repos/projects/u_net_baseline/quantitative_results.py:23\u001b[0m, in \u001b[0;36mmodel_metrics\u001b[0;34m(dataset, model, thresholds, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataset)):\n\u001b[1;32m     22\u001b[0m         image \u001b[39m=\u001b[39m dataset[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 23\u001b[0m         pred \u001b[39m=\u001b[39m model(image)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     24\u001b[0m         predictions\u001b[39m.\u001b[39mappend(pred)\n\u001b[1;32m     25\u001b[0m predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(predictions)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Repos/projects/u_net_baseline/unet_model.py:27\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup1(x4, x3)\n\u001b[1;32m     26\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup2(x, x2)\n\u001b[0;32m---> 27\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mup3(x, x1)\n\u001b[1;32m     28\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutc(x)\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msigmoid(x)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Repos/projects/u_net_baseline/unet_parts.py:90\u001b[0m, in \u001b[0;36mup.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39m# for padding issues, see \u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39m# https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m# https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\u001b[39;00m\n\u001b[1;32m     89\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x2, x1], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[1;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Repos/projects/u_net_baseline/unet_parts.py:22\u001b[0m, in \u001b[0;36mdouble_conv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set up model\n",
    "n_channels = 3 # RGB images\n",
    "model = UNet(n_channels)\n",
    "\n",
    "# set up datasets\n",
    "seed = config['seed']\n",
    "np.random.seed(seed) \n",
    "views = config['views']\n",
    "logging_path = config['log_path']\n",
    "if not os.path.exists(logging_path):\n",
    "    os.mkdir(logging_path)\n",
    "\n",
    "# make initial datasets for each view\n",
    "print('Loading datasets...')\n",
    "datasets = []\n",
    "for v in views:\n",
    "    datasets.append(BFSEvaluationDataset(config['images_path'],\n",
    "                                         config['gt_path'],\n",
    "                                         [v],\n",
    "                                         config['input_shape'],\n",
    "                                         config['output_shape']))\n",
    "# split each view into training and validation sets\n",
    "training_datasets, validation_datasets = [], []\n",
    "n_train = config['n_train'] # number of training images per view, remainder are validation\n",
    "save_train_indices = []\n",
    "save_val_indices = []\n",
    "for d in datasets:\n",
    "    train_indices = np.random.choice(np.arange(len(d)), size=n_train, replace=False)\n",
    "    val_indices = np.array([i for i in range(len(d)) if i not in train_indices])\n",
    "    save_train_indices.append([int(i) for i in train_indices])\n",
    "    save_val_indices.append([int(i) for i in val_indices])\n",
    "    training_datasets.append((d, train_indices))\n",
    "    validation_datasets.append((d, val_indices))\n",
    "# combine into one training, one validation dataset\n",
    "config['train_indices'] = save_train_indices\n",
    "config['validation_indices'] = save_val_indices\n",
    "print(training_datasets)\n",
    "train_dataset = BFSConcatDataset(training_datasets)\n",
    "val_dataset = BFSConcatDataset(validation_datasets) \n",
    "with open(os.path.join(logging_path, 'config.json'), 'w') as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "\n",
    "model, stat_dict = train(model, train_dataset, val_dataset, config)  \n",
    "# saving and clean-up\n",
    "torch.save(model, os.path.join(logging_path, 'model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(stat_dict['Training']['Precision'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Training precision')\n",
    "plt.subplot(222)\n",
    "plt.plot(stat_dict['Training']['Recall'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Training recall')\n",
    "plt.subplot(223)\n",
    "plt.plot(stat_dict['Training']['F-measure'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F-measure')\n",
    "plt.title('Training F-measure')\n",
    "plt.subplot(224)\n",
    "plt.plot(stat_dict['Training']['Blob recall'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Blob recall')\n",
    "plt.title('Training blob recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(stat_dict['Validation']['Precision'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Validation precision')\n",
    "plt.subplot(222)\n",
    "plt.plot(stat_dict['Validation']['Recall'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Validation recall')\n",
    "plt.subplot(223)\n",
    "plt.plot(stat_dict['Validation']['F-measure'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F-measure')\n",
    "plt.title('Validation F-measure')\n",
    "plt.subplot(224)\n",
    "plt.plot(stat_dict['Validation']['Blob recall'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Blob recall')\n",
    "plt.title('Validation blob recall')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantitative_results import *\n",
    "\n",
    "def evaluate_view(model, config, view):\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    thresholds = [0.5] # only checking one foreground probability threshold\n",
    "    test_dataset = BFSEvaluationDataset(config['images_path'], config['gt_path'], [test_view], config['input_shape'], config['output_shape'])\n",
    "    test_precision, test_recall, test_f_measure, test_blob_recall = model_metrics(test_dataset, model, thresholds, device)\n",
    "    return test_precision[0], test_recall[0], test_f_measure[0], test_blob_recall[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View: Almond at Washington East\n",
      "Precision = 0.000 | Recall = 0.000 | F-measure = 0.000 | Blob recall = 0.000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m test_views \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAlmond at Washington East\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBuffalo Grove at Deerfield North\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDeerfield at Saunders South\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDelany at Sunset North\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m test_view \u001b[39min\u001b[39;00m test_views:\n\u001b[0;32m----> 4\u001b[0m     p, r, f, b \u001b[39m=\u001b[39m evaluate_view(model, config, test_view)\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mView: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(test_view))\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPrecision = \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m | Recall = \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m | F-measure = \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m | Blob recall = \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(p, r, f, b))\n",
      "Cell \u001b[0;32mIn[50], line 7\u001b[0m, in \u001b[0;36mevaluate_view\u001b[0;34m(model, config, view)\u001b[0m\n\u001b[1;32m      5\u001b[0m thresholds \u001b[39m=\u001b[39m [\u001b[39m0.5\u001b[39m] \u001b[39m# only checking one foreground probability threshold\u001b[39;00m\n\u001b[1;32m      6\u001b[0m test_dataset \u001b[39m=\u001b[39m BFSEvaluationDataset(config[\u001b[39m'\u001b[39m\u001b[39mimages_path\u001b[39m\u001b[39m'\u001b[39m], config[\u001b[39m'\u001b[39m\u001b[39mgt_path\u001b[39m\u001b[39m'\u001b[39m], [test_view], config[\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m], config[\u001b[39m'\u001b[39m\u001b[39moutput_shape\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> 7\u001b[0m test_precision, test_recall, test_f_measure, test_blob_recall \u001b[39m=\u001b[39m model_metrics(test_dataset, model, thresholds, device)\n\u001b[1;32m      8\u001b[0m \u001b[39mreturn\u001b[39;00m test_precision[\u001b[39m0\u001b[39m], test_recall[\u001b[39m0\u001b[39m], test_f_measure[\u001b[39m0\u001b[39m], test_blob_recall[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Repos/projects/u_net_baseline/quantitative_results.py:23\u001b[0m, in \u001b[0;36mmodel_metrics\u001b[0;34m(dataset, model, thresholds, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataset)):\n\u001b[1;32m     22\u001b[0m         image \u001b[39m=\u001b[39m dataset[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 23\u001b[0m         pred \u001b[39m=\u001b[39m model(image)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     24\u001b[0m         predictions\u001b[39m.\u001b[39mappend(pred)\n\u001b[1;32m     25\u001b[0m predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(predictions)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Repos/projects/u_net_baseline/unet_model.py:23\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minc(x)\n\u001b[1;32m     22\u001b[0m x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown1(x1)\n\u001b[0;32m---> 23\u001b[0m x3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdown2(x2)\n\u001b[1;32m     24\u001b[0m x4 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown3(x3)\n\u001b[1;32m     25\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup1(x4, x3)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Repos/projects/u_net_baseline/unet_parts.py:58\u001b[0m, in \u001b[0;36mdown.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 58\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmpconv(x)\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Repos/projects/u_net_baseline/unet_parts.py:22\u001b[0m, in \u001b[0;36mdouble_conv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/active_learning/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# check the Annotations-GT or Annotations-Images folders for available scenes\n",
    "test_views = ['Almond at Washington East', 'Buffalo Grove at Deerfield North', 'Deerfield at Saunders South', 'Delany at Sunset North']\n",
    "for test_view in test_views:\n",
    "    p, r, f, b = evaluate_view(model, config, test_view)\n",
    "    print('View: {}'.format(test_view))\n",
    "    print('Precision = {:.3f} | Recall = {:.3f} | F-measure = {:.3f} | Blob recall = {:.3f}'.format(p, r, f, b))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, config, view, num_examples=5, indices=None):\n",
    "    # you may optionally specify the image indices within the dataset you would like to use\n",
    "    # otherwise, num_examples random images will be sampled from the dataset\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    dataset = test_dataset = BFSEvaluationDataset(config['images_path'], config['gt_path'], [view], config['input_shape'], config['output_shape'])\n",
    "    if indices is not None:\n",
    "        num_images = len(indices)\n",
    "        image_indices = indices\n",
    "    else:\n",
    "        num_images = num_examples\n",
    "        image_indices = np.random.choice(np.arange(len(dataset)), size=num_images, replace=False)\n",
    "    with torch.no_grad():\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for n in range(num_images):\n",
    "            image, gt = dataset[n]\n",
    "            image_numpy = image.permute(1, 2, 0).numpy()\n",
    "            image_tensor = image.unsqueeze(0).to(device)\n",
    "            prediction_tensor = model(image_tensor)\n",
    "            prediction_numpy = prediction_tensor.squeeze(0, 1).cpu().numpy()\n",
    "            gt_numpy = gt.squeeze(0).numpy()\n",
    "            plt.subplot(num_images, 3, 3*n+1)\n",
    "            plt.imshow(image_numpy)\n",
    "            plt.axis(False)\n",
    "            plt.title('Original image')\n",
    "            plt.subplot(num_images, 3, 3*n+2)\n",
    "            plt.imshow(gt_numpy, 'gray')\n",
    "            plt.axis(False)\n",
    "            plt.title('Ground-truth image')\n",
    "            plt.subplot(num_images, 3, 3*n+3)\n",
    "            plt.imshow(prediction_numpy, vmin=0, vmax=1, cmap='gray')\n",
    "            plt.axis(False)\n",
    "            plt.title('Model prediction image')\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = 'Almond at Washington East'\n",
    "num_examples = 5\n",
    "visualize_predictions(model, config, view, num_examples=num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
